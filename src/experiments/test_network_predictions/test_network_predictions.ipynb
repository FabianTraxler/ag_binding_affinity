{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {
                "trusted": true
            },
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": "WARNING:root:Pytorch pre-release version 1.12.1.post201 - assuming intent to test it\n"
                }
            ],
            "source": "from typing import Dict, List, Tuple, Union, Optional, Callable\n\nimport sys\nimport wandb\nimport numpy as np\nimport math\nimport pandas as pd\nimport os\nfrom copy import deepcopy\nfrom argparse import Namespace\nfrom pathlib import Path\nimport logging\nimport torch\nfrom scipy import stats\nfrom torch.optim import Adam\nfrom torch.utils.data import DataLoader as DL_torch\nfrom torch_geometric.data import DataLoader\nfrom tqdm import tqdm\nfrom sklearn.metrics import accuracy_score\nfrom functools import partial\n# import pdb\n\nfrom abag_affinity.dataset import AffinityDataset\nfrom abag_affinity.dataset.advanced_data_utils import complexes_from_dms_datasets, get_bucket_dataloader, load_datasets\nfrom abag_affinity.model import AffinityGNN, TwinWrapper\nfrom abag_affinity.train.wandb_config import configure\nfrom abag_affinity.utils.config import get_data_paths, read_config\nfrom abag_affinity.utils.visualize import plot_correlation\nfrom abag_affinity.utils.argparse_utils import read_args_from_file, parse_args\nfrom abag_affinity.train.utils import load_model\nfrom abag_affinity.model import regression_heads\n%load_ext autoreload\n%autoreload 2"
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": "args_file = \"base_args.txt\"\n\nsys.argv = sys.argv[:1]\nargs = parse_args(args_file=args_file)\n\nconfig = read_config(args.config_file)\n\nargs.batch_size = 1\nargs.cuda = False\nuse_cuda = False\ndevice = 'cpu'\n\ntrain_data, val_datas = load_datasets(config, args.target_dataset, args.validation_set, args)\n# test_sets = ['AntibodyBenchmark', 'SKEMPI.v2', 'abag_affinity']\ntest_sets = ['AntibodyBenchmark']\n\n# wandb_benchmark_log = run_and_log_benchmarks(model, args, wandb_inst)"
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": "def get_test_dataloader(test_set: str, args: Namespace) -> DataLoader:\n    if test_set == \"abag_affinity\":\n        summary_path, _ = get_data_paths(args.config, \"abag_affinity\")\n        summary_df = pd.read_csv(summary_path, index_col=0)\n        if args.validation_set is None:\n            summary_df = summary_df[summary_df[\"test\"]]\n        elif args.validation_set < 0:\n            summary_df = summary_df[summary_df[\"validation\"] != ((-1 * args.validation_set) - 1)]\n        else:\n            summary_df = summary_df[summary_df[\"validation\"] == args.validation_set]\n        pdb_ids = summary_df.index.tolist()\n    else:\n        pdb_ids = None\n    dataset = AffinityDataset(args.config, args.relaxed_pdbs, test_set, \"L2\",\n                              pdb_ids=pdb_ids,\n                              node_type=args.node_type,\n                              max_nodes=args.max_num_nodes,\n                              interface_distance_cutoff=args.interface_distance_cutoff,\n                              interface_hull_size=args.interface_hull_size,\n                              max_edge_distance=args.max_edge_distance,\n                              pretrained_model=args.pretrained_model,\n                              scale_values=args.scale_values,\n                              scale_min=args.scale_min,\n                              scale_max=args.scale_max,\n                              relative_data=False,\n                              save_graphs=args.save_graphs,\n                              force_recomputation=args.force_recomputation,\n                              preprocess_data=args.preprocess_graph,\n                              preprocessed_to_scratch=args.preprocessed_to_scratch,\n                              num_threads=args.num_workers,\n                              load_embeddings=None if not args.embeddings_type else (args.embeddings_type, args.embeddings_path)\n                              )\n\n    dataloader = DL_torch(dataset, num_workers=args.num_workers, batch_size=1,\n                          collate_fn=AffinityDataset.collate)\n\n    return dataloader"
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": "def get_dataset_predictions(model, dataloader, device):\n    y_data = []\n    y_pred = []\n    preds_per_residue = {}\n\n    for data in dataloader:\n        data_copy = deepcopy(data)\n        # print('data', data)\n        # print('data input size', data['input']['graph']['node'].x.shape)\n        # output = model(data['input'])\n        #\n        # print(output)\n        y_data.append(data_copy['input']['graph']['-log(Kd)'].item())\n        out = model(data_copy['input'])\n        y_pred.append(out['-log(Kd)'].item())\n\n        res_types = np.where(data['input']['graph']['node'].x[:, :20] == 1)[1]\n        # with np.printoptions(threshold=np.inf):\n        #     print('data graph', data['input']['graph']['node'].x.numpy())\n\n        out2 = model.graph_conv(data['input']['graph'])\n\n        x = out2[\"node\"].x\n\n        batch = regression_heads.get_node_batches(out2).to(x.device)\n\n        if model.regression_head.aggregation_method in [\"interface_sum\", \"interface_mean\", \"interface_size\"]:\n            # get interface edges\n            interface_node_indices = out2[\"node\", \"interface\", \"node\"].edge_index.view(-1).unique()\n            batch = batch[interface_node_indices]\n            x = x[interface_node_indices]\n            res = res_types[interface_node_indices]\n        # compute node-wise affinity contribution from graph embedding\n        for fc_layer in model.regression_head.fc_layers[:-1]:\n            x = fc_layer(x)\n            x = model.regression_head.activation(x)\n        x = model.regression_head.fc_layers[-1](x)\n        for i in range(res.shape[0]):\n            if res[i] in preds_per_residue.keys():\n                preds_per_residue[res[i].item()].append(x[i].item())\n            else:\n                preds_per_residue[res[i].item()] = [x[i].item()]\n    y_data = np.array(y_data)\n    y_pred = np.array(y_pred)\n\n    return preds_per_residue, y_data, y_pred"
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {
                "trusted": true
            },
            "outputs": [
                {
                    "ename": "NameError",
                    "evalue": "name 'meta_args' is not defined",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
                        "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m model\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m train_dataloader, val_dataloaders \u001b[38;5;241m=\u001b[39m get_bucket_dataloader(args, [train_data], val_datas)\n\u001b[0;32m----> 5\u001b[0m test_dataloaders \u001b[38;5;241m=\u001b[39m {test_set: get_test_dataloader(test_set, args) \u001b[38;5;28;01mfor\u001b[39;00m test_set \u001b[38;5;129;01min\u001b[39;00m \u001b[43mmeta_args\u001b[49m\u001b[38;5;241m.\u001b[39mtest_sets}\n",
                        "\u001b[0;31mNameError\u001b[0m: name 'meta_args' is not defined"
                    ]
                }
            ],
            "source": "model = AffinityGNN.load_from_checkpoint('/home/mihail/Documents/workspace/ag_binding_affinity/results/models/2023-12-20_19-00-38_fix_labels_abag_test/model.pt',\n                                         map_location='cpu')\nmodel.to('cpu')\ntrain_dataloader, val_dataloaders = get_bucket_dataloader(args, [train_data], val_datas)\ntest_dataloaders = {test_set: get_test_dataloader(test_set, args) for test_set in test_sets}"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": "dataloaders = {'Training': train_dataloader}\ndataloaders.update({'Validation' + str(i): val_dataloaders[i] for i in range(len(val_dataloaders))})\ndataloaders.update(test_dataloaders)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": "results = {}\nfor set_name, dataloader in dataloaders.items():\n    results[set_name] = {}\n    print('Per residue results for ' + set_name + ' dataset')\n    preds_per_residue, y_data, y_pred = get_dataset_predictions(model, dataloader, device)\n    results[set_name]['preds_per_residue'] = preds_per_residue\n    results[set_name]['y_data'] = y_data\n    results[set_name]['y_pred'] = y_pred\n\n    means_per_residue = {i: np.mean(preds_per_residue[i]) for i in preds_per_residue.keys()}\n    stds_per_residue = {i: np.std(preds_per_residue[i]) for i in preds_per_residue.keys()}\n    results[set_name]['average_residue_scores'] = means_per_residue\n    results[set_name]['standard_dev_per_residue'] = stds_per_residue\n\n    print('average_residue_scores', means_per_residue, '\\n')\n    print('standard dev per residue', stds_per_residue, '\\n')\n    print('standard dev across residues', np.std(list(means_per_residue.values())), '\\n')\n    print('\\n\\n')"
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {
                "trusted": true
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "0.1155047127776308 0.16149641672163967 0.1660761818237867\n0.16213827280404855 0.16570292877263126 0.1620403425991364\n"
                }
            ],
            "source": "train_mean = np.mean(results['Training']['y_data'])\n\nfor set_name in dataloaders.keys():\n    print(set_name, 'dataset')\n    results[set_name]['train_mean_rmse'] = np.sqrt(np.mean((results[set_name]['y_data'] - train_mean) ** 2))\n    results[set_name]['pred_rmse'] = np.sqrt(np.mean((results[set_name]['y_pred'] - results[set_name]['y_data']) ** 2))\n    print('train_mean_rmse', results[set_name]['train_mean_rmse'])\n    print('pred_rmse', results[set_name]['pred_rmse'])\n    print(\"\")"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": "mean_rmses = [results[set_name]['train_mean_rmse'] for set_name in dataloaders.keys()]\npred_rmses = [results[set_name]['pred_rmse'] for set_name in dataloaders.keys()]\nnp.corrcoef(mean_rmses, pred_rmses)\nprint('Correlation between train_mean_rmse and pred_rmse', np.corrcoef(mean_rmses, pred_rmses)[0, 1])"
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.16"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}