# TODO I am abusing the "data"/DMS folder maybe
import os.path
import pandas as pd
from collections import defaultdict
from pathlib import Path
import numpy as np


# INTERFACE_SIZE = 5
# INTERFACE_HULL_SIZE = 10
INTERFACE_SIZE = None  # We don't reduce to the interface
INTERFACE_HULL_SIZE = None


project_root = Path(workflow.basedir).parents[2] # two directories above - absolute paths not working
out_folder = project_root / "results" / "DMS"
base_structure_folder = out_folder / "prepared_renamed_pdbs"  # alternative: "predpared_af2_pdbs"
MUTATED_PDB_FOLDER = out_folder / "mutated"
RELAXED_PDB_FOLDER = out_folder / "mutated_relaxed"

metadata_file = project_root / "data" / "metadata_dms_studies.yaml"

# Generated from Moritz [[id:f1e36ab1-11c4-420d-9d0c-83f1e37f9311][Obtaining antibody binding data]]
DMS_DATA_FILE = project_root / "data" / "DMS" / "dms_curated.csv"
publications = pd.read_csv(DMS_DATA_FILE)["publication"].drop_duplicates()
# remove this publication because they have redundant information to mason21_comb_optim_therap_antib_by_predic_combined_H3_3. TODO really? why not augmentation?
publications = publications[~publications.isin(["mason21_comb_optim_therap_antib_by_predic_combined_H3_2", "mason21_comb_optim_therap_antib_by_predic_combined_H3_1"])]

#  TODO enable one by one
publications = [
    "madan21_mutat_hiv",
    "phillips21_bindin",  # ~500 antigen... <- let's see if it works
    "wu20_differ_ha_h3_h1",  # ~500 antigen
    "mason21_optim_therap_antib_by_predic_dms_H",
    "mason21_optim_therap_antib_by_predic_dms_L",
    "taft22_deep_mutat_learn_predic_ace2",
    "mason21_comb_optim_therap_antib_by_predic_combined_H3_3",
    "mason21_comb_optim_therap_antib_by_predic_combined_L3_3",
    "starr21_prosp_covid",
    "b.20_funct_screen_strat_engin_chimer",  # 644 antigen; remains disabled because they are larger than the others
    "wu17_in",
]


def relaxed_generated_pdbs(wildcards):
    all_files = []
    for publication in publications:
        df_fn = checkpoints.generate_summary_df.get(publication=publication).output[0]
        df = pd.read_csv(df_fn, index_col=0)
        if len(df) > 0:  # workaround
            all_files.extend([MUTATED_PDB_FOLDER / file_path for file_path in df["filename"]])  # TODO generate relaxed later!
    return all_files


rule all:
    input:
        expand(out_folder / "{publication}.csv", publication=publications),
        relaxed_generated_pdbs

include: "rules/standalone_helpers.smk"

rule subset_dms_data:
    """
    Reduce the number of data points "per dataset" to a maximum of 20,000 (see params)
    This is to make the whole preprocessing and trainign manageable

    TODO if possible, implement our training such that we can do *all* datapoints and don't need subsetting
    NOTE: I could have included this subsetting into the generate_summary_df rule fs
    """
    input:
        ancient(DMS_DATA_FILE)
    output:
        out_folder / "dms_subsampled.csv"
    params:
        num_max_datapoints=5000,  # 20000 takes 16 days. 200 is for testing. Let's use 5000 then (over the retreat)
        seed=42
    resources:
        mem_mb=10000
    run:
        # This function is very senstitive! Don't change it without being sure that the random state does not mess up
        import pandas as pd
        import numpy as np
        np.random.seed(params.seed)

        df = pd.read_csv(input[0])
        df.index.name = "index"
        def _subsample_complex(complex_df):
            """
            Randomly subsample the complex data such that the distribution is preserved
            """

            if len(complex_df) <= params.num_max_datapoints:
                return complex_df
            else:
                subsampled = complex_df.sample(params.num_max_datapoints, replace=False)
                # make sure to add all the data points that contain -log(Kd) values
                if complex_df["-log(Kd)"].isna().any():  # only add -log(Kd) values, if they are not present everywhere (e.g. excludes phillips)
                    log_kd_subset = complex_df[~complex_df["-log(Kd)"].isna()]
                    # subtract the ones already sampled
                    log_kd_subset = log_kd_subset[~log_kd_subset.index.isin(subsampled.index)]
                    subsampled = pd.concat([subsampled, log_kd_subset])

            # This is a bit hacky and done to preserve the random state such that we don't need to recompute all the PDBs
            if not complex_df["-log(Kd)"].isna().any():  # e.g. phillips
                subsampled = complex_df[complex_df["-log(Kd)"] > complex_df["-log(Kd)"].min()].iloc[:params.num_max_datapoints]  # remove the lowest values (because there are a lot and we can't really know how strong they really are)
            return subsampled

        filtered = df.groupby(["publication", "antibody", "antigen"]).apply(_subsample_complex)
        filtered.to_csv(output[0], index=False)


checkpoint generate_summary_df:
    input:
        dms_curated=rules.subset_dms_data.output[0],
        metadata_file=ancient(metadata_file),
    output:
        out_folder / "{publication,[^/]+}.csv"
    conda: project_root / "envs" / "generation_environment.yml"
    resources:
        mem_mb=10000
        # params:
        # metadata_file=metadata_file,
    script: "scripts/generate_summary.py"

rule get_initial_complex:
    """
    Download, rename chains and mutate as indicated in metadata file. We don't fix inserts, because the mutations still need to be applied.

    Also, reduce heavy and light chain to only contain the variable part (~110AAs each). Note: There are scFvs! (if L chain is missing, all is fine.)

    TODO could/should be merged with mutate_complex (because of DRYing mutation code)
    TODO relaxation should be done in a separate thread
    """
    input:
        metadata_file=ancient(metadata_file)
    output:
        base_structure_folder / "{publication}/{antibody,[^/]+}_{antigen,[^/]+}.pdb"
    params:
        max_l_len=115,
        max_h_len=125,
        project_root=project_root
    conda: "ag_binding_diffusion3"
    resources:
        mem_mb=10000
    threads: 1
    script: "scripts/get_complex.py"

rule mutate_complex:
    """
    Generate a single mutation
    """
    input:
        dms_curated=rules.subset_dms_data.output[0],
        pdb=base_structure_folder / "{publication}/{antibody}_{antigen}.pdb",
    output:
        mutated_pdb=temp(out_folder / "mutated_unfixedinsert" / "{publication}" / "{antibody,[^/]+}_{antigen,[^/]+}" / "{mut}.pdb")
    conda: project_root / "envs" / "generation_environment.yml"
    resources:
        mem_mb=8000
    threads: 1
    script: "scripts/mutate.py"

rule fixinsert:
    """
    We did not yet apply fixinsert, to make the mutations work. Doing it now!
    """
    input:
        out_folder / "mutated_unfixedinsert" /"{publication}" / "{antibody}_{antigen}" / "{mut}.pdb"
    output:
        protected(MUTATED_PDB_FOLDER / "{publication}" / "{antibody,[^/]+}_{antigen,[^/]+}" / "{mut}.pdb")
    resources:
        mem_mb=2000  # less should work too
    conda: project_root / "envs" / "generation_environment.yml"
    shell: # pdb_tidy is required again because the relaxation again changes some things
        "grep '^ATOM' {input} | pdb_sort | pdb_tidy | pdb_fixinsert > {output}"

rule relax_mutated:
    input:
        MUTATED_PDB_FOLDER /"{publication}" / "{antibody}_{antigen}" / "{mut}.pdb"
    output:
        pdb=protected(RELAXED_PDB_FOLDER / "{publication}" / "{antibody}_{antigen}" / "{mut}.pdb"),
        pdb_comments=RELAXED_PDB_FOLDER / "{publication}" / "{antibody}_{antigen}" / "{mut}.rosetta_comments"
    conda:
        project_root / "envs" / "generation_environment.yml"
    resources:
        mem_mb=4000,
    script:
        "../scripts/relax.py"
