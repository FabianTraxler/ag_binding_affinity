import os.path
import pandas as pd
from collections import defaultdict
from pathlib import Path

INTERFACE_SIZE = 5
INTERFACE_HULL_SIZE = 10


project_root = Path(workflow.basedir).parents[2] # two directories above - absolute paths not working
out_folder = os.path.join(project_root, "results/DMS/")

metadata_file = os.path.join(project_root, "data/metadata_dms_studies.yaml")
dms_out_summary_file = os.path.join(project_root, "results/DMS/dms_infos.csv")

dms_info_file = os.path.join(project_root, "data/DMS/dms_curated.csv")
info_df = pd.read_csv(dms_info_file)

# remove this publication because they have redundant information to mason21_comb_optim_therap_antib_by_predic_combined_H3_3
info_df = info_df[~info_df["publication"].isin(["mason21_comb_optim_therap_antib_by_predic_combined_H3_2", "mason21_comb_optim_therap_antib_by_predic_combined_H3_1"])]


# filter  data
#info_df = info_df[info_df["publication"].str[:7] != "mason21"]
#info_df = info_df[~info_df["publication"].isin(["taft22_deep_mutat_learn_predic_ace2"])]
#info_df = info_df[info_df["antibody"].isin(["regn10933"])]
info_df = info_df.reset_index(drop=True)

complexes = info_df.groupby(["publication", "antibody", "antigen"]).groups.keys()
publications, antibodies, antigens = list(zip(*complexes))

publication2antibodies = defaultdict(list)
publication2antigens = defaultdict(list)

for publi, antibody, antigen in zip(publications, antibodies, antigens):
    publication2antibodies[publi].append(antibody)
    publication2antigens[publi].append(antigen)


rule all:
    input:
        expand(out_folder + "{publication}.csv", publication=publications)
    output:
        os.path.join(out_folder, "mutated", "all_generated_files.txt")
    params:
        mutated_folder=os.path.join(out_folder, "mutated")
    shell: """
        cd {params.mutated_folder}
        find * -type f -name *.pdb > {output}
        """

rule get_complex:
    output:
        out_folder + "prepared_pdbs/{publication}/{antibody}_{antigen}.pdb"
    params:
        metadata_file=metadata_file,
        project_root=project_root
    conda: "../../../envs/generation_environment.yml"
    threads: 50
    script: "scripts/get_complex.py"

rule reduce2interface_hull:
    input:
        out_folder + "prepared_pdbs/{publication}/{antibody}_{antigen}.pdb"
    output:
        out_folder + "interface_hull_pdbs/{publication}/{antibody}_{antigen}.pdb"
    params:
        metadata_file=metadata_file,
        project_root=project_root,
        interface_size=INTERFACE_SIZE,
        interface_hull_size=INTERFACE_HULL_SIZE
    conda: "../../../envs/generation_environment.yml"
    resources:
        mem_mb=50000
    script: "scripts/reduce2interface_hull.py"


rule mutate_complex:
    input:
        out_folder + "interface_hull_pdbs/{publication}/{antibody}_{antigen}.pdb"
    output:
        out_folder + "mutated/{publication}/{antibody}_{antigen}.logs",
        out_folder + "mutated/{publication}/{antibody}_{antigen}.json"
        #out_folder + "mutated/{publication}/{antibody}_{antigen}/{mutation_code}.pdb"
    conda: "../../../envs/generation_environment.yml"
    resources:
        mem_mb=50000
    threads: 100
    params:
        info_df=info_df,
        mutation_out_folder = out_folder + "mutated"
    script: "scripts/mutate.py"


rule generate_summary_df:
    input:
        lambda wildcards: expand(out_folder + "mutated/{publication}/{antibody}_{antigen}.logs", zip, publication=[wildcards.publication] * len(publication2antibodies[wildcards.publication]), antibody=publication2antibodies[wildcards.publication], antigen=publication2antigens[wildcards.publication])
    output:
        out_folder + "{publication}.csv"
    conda: "../../../envs/generation_environment.yml"
    resources:
        mem_mb=10000
    params:
        info_df=info_df,
        metadata_file=metadata_file,
    script: "scripts/generate_summary.py"