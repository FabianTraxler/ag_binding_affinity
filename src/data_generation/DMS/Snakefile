# TODO I am abusing the "data"/DMS folder maybe
import os.path
import pandas as pd
from collections import defaultdict
from pathlib import Path
import numpy as np


# INTERFACE_SIZE = 5
# INTERFACE_HULL_SIZE = 10
INTERFACE_SIZE = None  # We don't reduce to the interface
INTERFACE_HULL_SIZE = None


project_root = Path(workflow.basedir).parents[2] # two directories above - absolute paths not working
out_folder = project_root / "results" / "DMS"
base_structure_folder = out_folder / "prepared_renamed_pdbs"  # alternative: "predpared_af2_pdbs"
MUTATED_PDB_FOLDER = out_folder / "mutated"
RELAXED_PDB_FOLDER = out_folder / "mutated_relaxed"

metadata_file = project_root / "data" / "metadata_dms_studies.yaml"

# Generated from Moritz [[id:f1e36ab1-11c4-420d-9d0c-83f1e37f9311][Obtaining antibody binding data]]
DMS_DATA_FILE = project_root / "data" / "DMS" / "dms_curated.csv"
publications = pd.read_csv(DMS_DATA_FILE)["publication"].drop_duplicates()
# remove this publication because they have redundant information to mason21_comb_optim_therap_antib_by_predic_combined_H3_3. TODO really? why not augmentation?
publications = publications[~publications.isin(["mason21_comb_optim_therap_antib_by_predic_combined_H3_2", "mason21_comb_optim_therap_antib_by_predic_combined_H3_1"])]

#  TODO enable one by one
publications = [
    "madan21_mutat_hiv",
    "phillips21_bindin",  # ~500 antigen... <- let's see if it works
    # "wu20_differ_ha_h3_h1",  # ~500 antigen
    # "mason21_optim_therap_antib_by_predic_dms_H",
    # "mason21_optim_therap_antib_by_predic_dms_L",
    # "taft22_deep_mutat_learn_predic_ace2",
    # "mason21_comb_optim_therap_antib_by_predic_combined_H3_3",
    # "mason21_comb_optim_therap_antib_by_predic_combined_L3_3",
    # "starr21_prosp_covid",
    # "b.20_funct_screen_strat_engin_chimer",  # 644 antigen
    # "wu17_in",
]
# now: why the rename fails?


def relaxed_generated_pdbs(wildcards):
    all_files = []
    for publication in publications:
        df_fn = checkpoints.generate_summary_df.get(publication=publication).output[0]
        df = pd.read_csv(df_fn, index_col=0)
        if len(df) > 0:  # workaround
            all_files.extend([MUTATED_PDB_FOLDER / file_path for file_path in df["filename"]])  # TODO generate relaxed later!
    return all_files


rule all:
    input:
        expand(out_folder / "{publication}.csv", publication=publications),
        relaxed_generated_pdbs

rule subset_dms_data:
    """
    Reduce the number of data points "per dataset" to a maximum of 20,000 (see params)
    This is to make the whole preprocessing and trainign manageable

    TODO if possible, implement our training such that we can do *all* datapoints and don't need subsetting
    NOTE: I could have included this subsetting into the generate_summary_df rule fs
    """
    input:
        DMS_DATA_FILE
    output:
        out_folder / "dms_subsampled.csv"
    params:
        num_max_datapoints=5000,  # 20000 takes 16 days. 200 is for testing. Let's use 5000 then (over the retreat)
        seed=42
    resources:
        mem_mb=10000
    run:
        import pandas as pd
        import numpy as np
        np.random.seed(params.seed)

        df = pd.read_csv(input[0])
        df.index.name = "index"
        def _subsample_complex(complex_df):
            """
            Randomly subsample the complex data such that the distribution is preserved
            """
            if len(complex_df) <= params.num_max_datapoints:
                return complex_df
            else:
                return complex_df.sample(params.num_max_datapoints, replace=False)

        filtered = df.groupby(["publication", "antibody", "antigen"]).apply(_subsample_complex)
        filtered.to_csv(output[0], index=False)


checkpoint generate_summary_df:
    input:
        dms_curated=rules.subset_dms_data.output[0],
        metadata_file=ancient(metadata_file),
    output:
        out_folder / "{publication}.csv"
    conda: project_root / "envs" / "generation_environment.yml"
    resources:
        mem_mb=10000
        # params:
        # metadata_file=metadata_file,
    script: "scripts/generate_summary.py"

rule get_initial_complex:
    """
    Download, rename chains and mutate as indicated in metadata file. We don't fix inserts, because the mutations still need to be applied.

    Also, reduce heavy and light chain to only contain the variable part (~110AAs each). Note: There are scFvs! (if L chain is missing, all is fine.)

    TODO could/should be merged with mutate_complex (because of DRYing mutation code)
    TODO relaxation should be done in a separate thread
    """
    input:
        metadata_file=ancient(metadata_file)
    output:
        base_structure_folder / "{publication}/{antibody,[^/]+}_{antigen,[^/]+}.pdb"
    params:
        max_l_len=115,
        max_h_len=125,
        project_root=project_root
    conda: "ag_binding_diffusion3"
    resources:
        mem_mb=10000
    threads: 1
    script: "scripts/get_complex.py"

rule mutate_complex:
    """
    Generate a single mutation
    """
    input:
        dms_curated=rules.subset_dms_data.output[0],
        pdb=base_structure_folder / "{publication}/{antibody}_{antigen}.pdb",
    output:
        mutated_pdb = out_folder / "mutated_unfixedinsert" / "{publication}" / "{antibody,[^/]+}_{antigen,[^/]+}" / "{mut}.pdb"
    conda: project_root / "envs" / "generation_environment.yml"
    resources:
        mem_mb=8000
    threads: 1
    script: "scripts/mutate.py"

rule fixinsert:
    """
    We did not yet apply fixinsert, to make the mutations work. Doing it now!
    """
    input:
        out_folder / "mutated_unfixedinsert" /"{publication}" / "{antibody}_{antigen}" / "{mut}.pdb"
    output:
        MUTATED_PDB_FOLDER / "{publication}" / "{antibody,[^/]+}_{antigen,[^/]+}" / "{mut}.pdb"
    resources:
        mem_mb=2000  # less should work too
    conda: project_root / "envs" / "generation_environment.yml"
    shell: # pdb_tidy is required again because the relaxation again changes some things
        "grep '^ATOM' {input} | pdb_sort | pdb_tidy | pdb_fixinsert > {output}"

rule relax_mutated:
    input:
        MUTATED_PDB_FOLDER /"{publication}" / "{antibody}_{antigen}" / "{mut}.pdb"
    output:
        pdb=RELAXED_PDB_FOLDER / "{publication}" / "{antibody}_{antigen}" / "{mut}.pdb",
        pdb_comments=RELAXED_PDB_FOLDER / "{publication}" / "{antibody}_{antigen}" / "{mut}.rosetta_comments"
    conda:
        project_root / "envs" / "generation_environment.yml"
    resources:
        mem_mb=4000,
    script:
        "../scripts/relax.py"
