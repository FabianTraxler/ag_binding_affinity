# TODO I am abusing the "data"/DMS folder maybe
import os.path
import pandas as pd
from collections import defaultdict
from pathlib import Path
import numpy as np


# INTERFACE_SIZE = 5
# INTERFACE_HULL_SIZE = 10
INTERFACE_SIZE = None  # We don't reduce to the interface
INTERFACE_HULL_SIZE = None


project_root = Path(workflow.basedir).parents[2] # two directories above - absolute paths not working
out_folder = project_root / "results" / "DMS"
base_structure_folder = out_folder / "prepared_renamed_pdbs"  # alternative: "predpared_af2_pdbs"
MUTATED_PDB_FOLDER = out_folder / "mutated"
RELAXED_PDB_FOLDER = out_folder / "mutated_relaxed"

metadata_file = project_root / "data" / "metadata_dms_studies.yaml"

# Generated from Moritz [[id:f1e36ab1-11c4-420d-9d0c-83f1e37f9311][Obtaining antibody binding data]]
DMS_DATA_FILE = project_root / "data" / "DMS" / "dms_curated.csv"
publications = pd.read_csv(DMS_DATA_FILE)["publication"].drop_duplicates()
# remove this publication because they have redundant information to mason21_comb_optim_therap_antib_by_predic_combined_H3_3. TODO really? why not augmentation?
publications = publications[~publications.isin(["mason21_comb_optim_therap_antib_by_predic_combined_H3_2", "mason21_comb_optim_therap_antib_by_predic_combined_H3_1"])]


def relaxed_generated_pdbs(wildcards):
    all_files = []
    for publication in np.unique(publications):
        df_fn = checkpoints.generate_summary_df.get(publication=publication).output[0]
        df = pd.read_csv(df_fn, index_col=0)
        if len(df) > 0:  # workaround
            all_files.extend([MUTATED_PDB_FOLDER / file_path for file_path in df["filename"]])  # TODO generate relaxed later!
    return all_files


rule all:
    input:
        expand(out_folder / "{publication}.csv", publication=publications),
        relaxed_generated_pdbs

rule subset_dms_data:
    """
    Reduce the number of data points "per dataset" to a maximum of 20,000 (see params)
    This is to make the whole preprocessing and trainign manageable

    TODO if possible, implement our training such that we can do *all* datapoints and don't need subsetting
    NOTE: I could have included this subsetting into the generate_summary_df rule fs
    """
    input:
        ancient(DMS_DATA_FILE)
    output:
        out_folder / "dms_subsampled.csv"
    params:
        num_max_datapoints=5000,  # 20000 takes 16 words. 200 is for testing. Let's use 5000 then (over the retreat)
        seed=42
    run:
        import pandas as pd
        import numpy as np
        np.random.seed(params.seed)

        df = pd.read_csv(input[0])
        df.index.name = "index"
        def _subsample_complex(complex_df):
            """
            Randomly subsample the complex data such that the distribution is preserved
            """
            if len(complex_df) <= params.num_max_datapoints:
                return complex_df
            else:
                return complex_df.sample(params.num_max_datapoints, replace=False)

        filtered = df.groupby(["publication", "antibody", "antigen"]).apply(_subsample_complex)

        filtered.to_csv(output[0], index=False)


checkpoint generate_summary_df:
    input:
        dms_curated=rules.subset_dms_data.output[0],
        metadata_file=ancient(metadata_file),
    output:
        out_folder / "{publication}.csv"
    conda: "../../../envs/generation_environment.yml"
    resources:
        mem_mb=10000
        # params:
        # metadata_file=metadata_file,
    script: "scripts/generate_summary.py"

rule get_initial_complex:
    """
    Download and mutate PDB as indicated in metadata file

    TODO could/should be merged with mutate_complex
    TODO relaxation should be done in a separate thread
    """
    input:
        metadata_file=ancient(metadata_file)
    output:
        out_folder / "prepared_pdbs" / "{publication}/{antibody,[^/]+}_{antigen,[^/]+}.pdb"
    params:
        project_root=project_root
    conda: "../../../envs/generation_environment.yml"
    threads: 1
    script: "scripts/get_complex.py"

rule rename_chains:
    """
    Just copies the manually prepared PDBs from prepared_pdbs and renames their chains. We don't clean them, because the mutations still need to be applied.

    Note: we could have alternatively used the metdadata information in the rule `input_pdb_to_fasta`, to order the chains correctly.
    """
    input:
        pdb=out_folder / "prepared_pdbs" / "{publication}" / "{complex}.pdb",  # TODO prepared_pdbs/ should go into data/ maybe. Or where do I have code to generate them?
        metadata=ancient(metadata_file)
    output:
        base_structure_folder / "{publication}" / "{complex}.pdb"
    conda:
        "ag_binding_diffusion3"
    script:
        "scripts/rename_chains.py"

# rule reduce2interface_hull:
#     """
#     TODO exploit/modify this function to reduce protein size in another manner (for those proteins that are too large).
#     Specifically, reduce heavy and light chain to only contain the variable part (~110AAs each). Note: There are scFvs! (if L chain is missing, all is fine.)
#     """
#     input:
#         base_structure_folder / "{publication}/{antibody}_{antigen}.pdb"
#     output:
#         out_folder / "reduced_sized_pdbs/{publication,[^/]+}/{antibody,[^/]+}_{antigen,[^/]+}.pdb"
#     params:
#         metadata_file=metadata_file,
#         interface_size=INTERFACE_SIZE,
#         interface_hull_size=INTERFACE_HULL_SIZE
#     conda: "../../../envs/generation_environment.yml"
#     resources:
#         mem_mb=50000
#     script: "scripts/reduce2interface_hull.py"

rule mutate_complex:
    """
    Generate a single mutation
    """
    input:
        dms_curated=rules.subset_dms_data.output[0],
        pdb=base_structure_folder / "{publication}/{antibody}_{antigen}.pdb",
    output:
        mutated_pdb = out_folder / "mutated_unfixedinsert" / "{publication}" / "{antibody,[^/]+}_{antigen,[^/]+}" / "{mut}.pdb"
    conda: "../../../envs/generation_environment.yml"
    resources:
        mem_mb=50000
    threads: 1
    script: "scripts/mutate.py"

rule fixinsert:
    """
    We did not yet apply fixinsert, to make the mutations work. Doing it now!
    """
    input:
        out_folder / "mutated_unfixedinsert" /"{publication}" / "{antibody}_{antigen}" / "{mut}.pdb"
    output:
        MUTATED_PDB_FOLDER / "{publication}" / "{antibody,[^/]+}_{antigen,[^/]+}" / "{mut}.pdb"
    conda: "../../../envs/generation_environment.yml"
    shell: # pdb_tidy is required again because the relaxation again changes some things
        "grep '^ATOM' {input} | pdb_sort | pdb_tidy | pdb_fixinsert > {output}"

rule relax_mutated:
    input:
        MUTATED_PDB_FOLDER /"{publication}" / "{antibody}_{antigen}" / "{mut}.pdb"
    output:
        pdb=RELAXED_PDB_FOLDER / "{publication}" / "{antibody}_{antigen}" / "{mut}.pdb",
        pdb_comments=RELAXED_PDB_FOLDER / "{publication}" / "{antibody}_{antigen}" / "{mut}.rosetta_comments"
    conda:
        project_root / "envs" / "generation_environment.yml"
    resources:
        mem_mb=4000,
    script:
        "../scripts/relax.py"
