# TODO it would have been best to just generate everything within data_generation/abag_affinity_dataset/scripts/generate_dataset.py!!!!

import os
import pandas as pd

from os.path import join
from pathlib import Path

DATASET_NAME = "antibody_benchmark"

project_root = Path(workflow.basedir).parents[2] # two directories above - absolute paths not working
resource_folder = os.path.join(project_root, "resources")
results_folder = os.path.join(project_root, "results")

dataset_resource_folder = os.path.join(resource_folder, DATASET_NAME)
abdb_resource_pdb_folder = os.path.join(resource_folder, "AbDb", "pdbs")
dataset_results_folder = os.path.join(results_folder, DATASET_NAME)

pdb_relaxed_folder = os.path.join(dataset_results_folder, "pdbs_relaxed")
pdb_bound_folder = os.path.join(dataset_results_folder, "pdbs_bound")
pdb_folder = os.path.join(dataset_resource_folder, "pdbs")

def gater_pdb_ids(wildcards):
    metadata_fn = checkpoints.clean_metadata.get(**wildcards).output[0]
    df = pd.read_csv(metadata_fn)
    # pdb_id = glob_wildcards(f"{pdb_folder}/{{pdb_id}}_r_b.pdb").pdb_id
    return expand(f"{pdb_bound_folder}/{{pdb_id}}.pdb", pdb_id=df["pdb"].str.upper().tolist())


rule all:
    input:
        gater_pdb_ids,
        join(dataset_results_folder, "benchmark.csv")

rule download_repo:
    output:
        join(dataset_resource_folder, "README.md")
    params:
        resource_folder=resource_folder,
    shell:
        "scripts/download_repo.sh {params.resource_folder}"


rule download_metadata:
    input:
        rules.download_repo.output
    output:
        join(dataset_resource_folder, "antibody_benchmark_cases.xlsx")
    params:
        dataset_resource_folder=dataset_resource_folder
    shell:
        "scripts/download_metadata.sh {params.dataset_resource_folder}"


checkpoint clean_metadata:
    input:
        xlsx=rules.download_metadata.output,
        pdb_dir=directory(abdb_resource_pdb_folder)
    output:
        csv=join(dataset_results_folder, "benchmark.csv")
    conda:
        "ag_binding_diffusion3"  # conda env loading does not work with "script:"  https://github.com/snakemake/snakemake/issues/1811
    script:
        "scripts/clean_metadata.py"


rule bind_structures:
    input:
        # antibody=join(dataset_resource_folder, "pdbs/{pdb_id}_r_b.pdb"),
        # antigen=join(dataset_resource_folder, "pdbs/{pdb_id}_l_b.pdb"),
        input_pdb=join(abdb_resource_pdb_folder, "{pdb_id}_1.pdb"),
        # csv=rules.clean_metadata.output.csv
    output:
        join(pdb_bound_folder, "{pdb_id}.pdb")
    # resources:
    #     mem_mb=40000,
    #     slurm="cpus-per-task=5 gres=gpu:a100:1 qos=a100 partition=gpu"
    conda:
        "ag_binding_diffusion3"   # conda env loading does not work with "script:"  https://github.com/snakemake/snakemake/issues/1811
    # conda:
    #     join(project_root, "envs/generation_environment.yml")
    shell: """python scripts/clean_pdb.py {input} {output}"""
    # shell: """
    #     # just copy from abdb and everybody is happy
    #     cp {input.input_pdb} {output}
    # """
    # run:  Would still require to reduce the antibody to the variable fragment

        # import pandas as pd

        # import subprocess

        # df = pd.read_csv(input.csv, index_col="pdb")
        # chain_infos = eval(df.loc[wildcards.pdb_id.lower(), "chain_infos"])  # only for replacement

        # replace_chain_commands="| ".join([f"pdb_rplchain -{old_chain}:{new_chain}" for old_chain, new_chain in chain_infos.items()])
        # cmd = f"cat {input.antibody} {input.antigen} | pdb_sort | pdb_tidy | pdb_delhetatm | {replace_chain_commands} > {output}"
        # print(cmd)
        # subprocess.run(cmd, shell=True, check=True)


rule relax_structures:
    input:
        join(pdb_bound_folder, "{pdb_id}.pdb")
    output:
        join(pdb_relaxed_folder, "{pdb_id}.pdb")
    conda:
        join(project_root, "envs/generation_environment.yml")
    script:
        "../scripts/relax.py"
