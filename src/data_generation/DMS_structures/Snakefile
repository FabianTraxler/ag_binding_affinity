import glob
import yaml
from pathlib import Path
import itertools

# old todos:
# TODO make this a submodule of the other DMS pipeline
# TODO We should relax. Sidechain packing is too weak <- we anyways use AF2 now
# TODO for now, we rely on manually generated files. ultimately, we want them to be generated with AF2. In the ideal scenario, they should be pulled automatically, using my metadata file
# TODO What to do with dms_curated.csv? Generated here: /home/moritz/wiki/roam/20220805103344-obtaining_antibody_binding_data.org

project_root = Path(workflow.basedir).parents[2] # two directories above - absolute paths not working
log_dir = project_root / "logs"
results_dir = project_root / "results"
dms_results = results_dir / "DMS"

METADATA_FILE = project_root / "data" / "metadata_dms_studies.yaml"
METADATA = yaml.safe_load(METADATA_FILE.read_text())
PUBLICATION_COMPLEXES = [(publication, complex['antibody']['name'] + '_' + complex['antigen']['name']) for publication, publication_data in METADATA.items() for complex in publication_data['complexes']]

rule all:
    input:
        # all
        # [d.replace('input_pdb', 'output_pdb').replace('.pdb', f'_{i}.pdb') for d in glob.glob('input_pdb/*/*.pdb') for i in range(1, 6)]
        # scores
        dms_results / "structure_prediction_matching" / "all_scores.csv",
        dms_results / "structure_prediction_matching" / 'all_scores.png',
        # scores/{publication}/{complex}_{rank}.score

        # [d.replace('input_pdb', 'output_pdb') for d in glob.glob('input_pdb/*/*.pdb')]
        # [d.replace('input_pdb', 'input_fasta').replace('.pdb', '.fasta') for d in glob.glob('input_pdb/*/*.pdb')]

rule rename_chains:
    """
    Just copies the manually prepared PDBs from prepared_pdbs and renames their chains. We don't clean them, because the mutations still need to be applied.

    Note: we could have alternatively used the metdadata information in the rule `input_pdb_to_fasta`, to order the chains correctly.
    """
    input:
        pdb=dms_results / "prepared_pdbs" / "{publication}" / "{complex}.pdb",  # TODO prepared_pdbs/ should go into data/ maybe. Or where do I have code to generate them?
        metadata=ancient(METADATA_FILE)
    output:
        dms_results / "prepared_renamed_pdbs" / "{publication}" / "{complex}.pdb"
    conda:
        "ag_binding_diffusion3"
    script:
        "scripts/rename_chains.py"

# TODO make sure that the chain order is correct (L(H)A(BCDE)) such that we can assign the chains later again
# TODO make sure that the residue_index is retained!
# TODO final output: prepared_cleaned_pdbs
rule input_pdb_to_fasta:
    input:
        dms_results / "prepared_renamed_pdbs" / "{publication}" / "{complex}.pdb"
    output:
        fasta=dms_results / "prepared_fasta" / "{publication}" / "{complex}.fasta"
    run:
        from Bio import SeqIO

        with open(input[0], 'r') as pdb_file, open(output["fasta"], 'w') as fasta_file:
            fasta_file.write(f'>{wildcards.publication}__{wildcards.complex}\n')
            for i, record in enumerate(SeqIO.parse(pdb_file, 'pdb-atom')):
                if i > 0:
                    fasta_file.write(':')
                fasta_file.write(str(record.seq))

rule get_template:
    """
    if complex contains :, trim it (to allow for mutations) TODO improve this comment

    """
    input:
        METADATA_FILE
    output:
        template_dir=directory(dms_results / "structure_templates" / "{publication}" / "{complex}")
    conda:
        "ag_binding_diffusion3"
    script:
        "scripts/get_template.py"

# complex contains publication name. why?

# TODO fix it in the end to retain the correct chains and residue_indices
AF_OUTPUT_PATH = dms_results / "af_prediction" / "{{publication}}" / "{{complex}}_{rank}.pdb"
rule predict_structures:
    input:
        fasta=rules.input_pdb_to_fasta.output.fasta,
        template_dir=rules.get_template.output.template_dir
    output:
        af_output=expand(AF_OUTPUT_PATH, rank=list(range(1, 6))),
    params:
        output_folder=lambda wildcards, input, output: Path(output["af_output"][0]).parent,
    log:
        log_dir / "af_prediction" / "{publication}" / "{complex}.log"
    threads: 10  # make sure that structure prediction does not go in parallel due to memory consumption
    resources:
        mem_mb=40000,
        slurm="cpus-per-task=5 gres=gpu:a100:1 qos=a100 partition=gpu"
    conda:
        "colabfold_multimer_patch"  # See readme on how this needs to be generated
    shell:
        '''
        # Delete old AF2 results
        rm {params.output_folder}/{wildcards.publication}__{wildcards.complex}_unrelaxed_rank_00?_alphafold2_multimer_v3_model_?_seed_000.pdb 2>/dev/null || true

        colabfold_batch --overwrite-existing-results --msa-mode single_sequence --rank multimer --templates --custom-template-path {input.template_dir} {input.fasta} {params.output_folder} 2>&1 | tee {log}
        cd {params.output_folder}
        # The file name is determined by the FASTA header and other factors
        for rank in `seq 1 5`; do  # TODO we should define the loop over output.af_output and output.link. BUT not sure if we need this link anyways
            ln -s {wildcards.publication}__{wildcards.complex}_unrelaxed_rank_00${{rank}}_alphafold2_multimer_v3_model_?_seed_000.pdb {wildcards.complex}_${{rank}}.pdb
        done
        '''

include: "rules/validation.smk"
include: "rules/scores.smk"
